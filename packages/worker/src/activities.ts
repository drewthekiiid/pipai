/**
 * PIP AI Temporal Activities
 * Core processing activities for document analysis and AI workflows
 */

import { GetObjectCommand, S3Client } from '@aws-sdk/client-s3';
import { Context } from '@temporalio/activity';
import { config } from 'dotenv';
import * as fs from 'fs/promises';
import * as path from 'path';
import { createPDFToImagesClient } from './pdf-to-images-client.js';
import { createUnstructuredClient } from './unstructured-client.js';

// Load environment variables
config({ path: '../../.env' });
config({ path: '../../.env.local', override: true });

// Activity context helper
function getActivityInfo() {
  const context = Context.current();
  return {
    activityId: context.info.activityId,
    taskToken: context.info.taskToken,
    workflowId: context.info.workflowExecution.workflowId,
  };
}

// Types
interface DownloadFileInput {
  fileUrl: string;
  userId: string;
  analysisId: string;
}

interface DownloadFileResult {
  filePath: string;
  fileType: string;
  fileName: string;
  fileSize: number;
  tempDir: string;
  fileContent: string; // Empty for large files to avoid Temporal's 2MB limit
  originalUrl: string; // Original download URL for reference
  presignedUrl: string; // Presigned URL for cross-activity access (avoids 2MB limit)
}

interface ExtractTextInput {
  filePath: string;
  fileType: string;
  options?: {
    extractImages?: boolean;
    detectLanguage?: boolean;
  };
}

interface ExtractTextResult {
  extractedText: string;
  metadata: {
    pageCount?: number;
    language?: string;
    imageCount?: number;
    processingTime: number;
    tables?: Array<{
      pageNumber: number;
      html: string;
      text: string;
    }>;
    images?: Array<{
      pageNumber: number;
      imagePath: string;
      caption?: string;
    }>;
    layout?: {
      headers: string[];
      sections: string[];
      lists: string[];
    };
    processingMethod?: 'unstructured';
  };
}

interface GenerateEmbeddingsInput {
  text: string;
  userId: string;
}

interface GenerateEmbeddingsResult {
  embeddings: number[];
  dimensions: number;
  model: string;
}

interface AIAnalysisInput {
  text: string;
  analysisType: 'document' | 'code' | 'data' | 'image';
  metadata?: Record<string, any>;
}

interface AIAnalysisResult {
  summary: string;
  insights: string[];
  keyTopics: string[];
  sentiment?: string;
  complexity?: number;
}

/**
 * Download and validate file from URL or cloud storage
 */
export async function downloadFileActivity(input: DownloadFileInput): Promise<DownloadFileResult> {
  const { activityId } = getActivityInfo();
  console.log(`[${activityId}] Downloading file: ${input.fileUrl}`);

  try {
    // Create unique temp directory with activity ID to prevent conflicts
    const uniqueTempId = `${input.analysisId}_${activityId}_${Date.now()}`;
    const tempDir = path.join('/tmp', uniqueTempId);
    await fs.mkdir(tempDir, { recursive: true });
    console.log(`[${activityId}] Created unique temp directory: ${tempDir}`);

    let fileContent: string;
    let fileName: string;
    let fileBuffer: Buffer | null = null;
    
    if (input.fileUrl.startsWith('http')) {
      // Check if this is an S3 URL
      if (input.fileUrl.includes('.s3.') || input.fileUrl.includes('s3.amazonaws.com')) {
        // Handle S3 download with AWS SDK
        console.log(`[${activityId}] Detected S3 URL, using AWS SDK for download`);
        
        try {
          // Parse S3 URL to extract bucket and key
          const s3UrlMatch = input.fileUrl.match(/https:\/\/([^.]+)\.s3(?:\.([^.]+))?\.amazonaws\.com\/(.+)/);
          if (!s3UrlMatch) {
            throw new Error(`Invalid S3 URL format: ${input.fileUrl}`);
          }
          
          const [, bucket, region, key] = s3UrlMatch;
          console.log(`[${activityId}] S3 Details - Bucket: ${bucket}, Key: ${key}`);
          
          // Configure S3 client
          const s3Client = new S3Client({
            region: region || process.env.AWS_REGION || 'us-east-1',
            credentials: {
              accessKeyId: process.env.AWS_ACCESS_KEY_ID!,
              secretAccessKey: process.env.AWS_SECRET_ACCESS_KEY!,
            },
          });
          
          console.log(`[${activityId}] Sending S3 GetObject request...`);
          const getObjectCommand = new GetObjectCommand({
            Bucket: bucket,
            Key: key,
          });
          
          const response = await s3Client.send(getObjectCommand);
          console.log(`[${activityId}] S3 response received, processing stream...`);
          
          // Handle different stream types from AWS SDK v3
          if (response.Body) {
            const chunks: Buffer[] = [];
            
            if (response.Body instanceof Uint8Array) {
              // Direct binary data
              fileBuffer = Buffer.from(response.Body);
            } else if (typeof response.Body === 'string') {
              // String data
              fileBuffer = Buffer.from(response.Body, 'utf-8');
            } else if (response.Body && typeof (response.Body as any)[Symbol.asyncIterator] === 'function') {
              // Async iterable stream (AWS SDK v3)
              for await (const chunk of response.Body as AsyncIterable<Uint8Array>) {
                chunks.push(Buffer.from(chunk));
              }
              fileBuffer = Buffer.concat(chunks);
            } else if (response.Body && typeof (response.Body as any).on === 'function') {
              // Node.js Readable stream
              const stream = response.Body as any;
              for await (const chunk of stream) {
                chunks.push(Buffer.isBuffer(chunk) ? chunk : Buffer.from(chunk));
              }
              fileBuffer = Buffer.concat(chunks);
            } else {
              throw new Error(`Unsupported S3 response body type: ${typeof response.Body}`);
            }
            
            // Extract filename from Content-Disposition or URL
            fileName = response.ContentDisposition?.match(/filename="([^"]+)"/)?.[1] || 
                     path.basename(key) || 
                     `downloaded_file_${Date.now()}`;
                     
            console.log(`[${activityId}] S3 download successful: ${fileBuffer.length} bytes, filename: ${fileName}`);
          } else {
            throw new Error('Empty response body from S3');
          }
        } catch (error) {
          console.error(`[${activityId}] S3 download failed:`, error);
          throw new Error(`Failed to download from S3: ${error instanceof Error ? error.message : String(error)}`);
        }
      } else {
        // Handle regular HTTP download
        console.log(`[${activityId}] Downloading from HTTP URL`);
        const response = await fetch(input.fileUrl);
        if (!response.ok) {
          throw new Error(`HTTP error! status: ${response.status}`);
        }
        
        const arrayBuffer = await response.arrayBuffer();
        fileBuffer = Buffer.from(arrayBuffer);
        fileName = path.basename(new URL(input.fileUrl).pathname) || `downloaded_file_${Date.now()}`;
        console.log(`[${activityId}] HTTP download successful: ${fileBuffer.length} bytes`);
      }
    } else {
      // Handle direct file path (local file)
      console.log(`[${activityId}] Reading local file: ${input.fileUrl}`);
      fileBuffer = await fs.readFile(input.fileUrl);
      fileName = path.basename(input.fileUrl);
      console.log(`[${activityId}] Local file read successful: ${fileBuffer.length} bytes`);
    }

    // Write file to unique temp directory
    const fullFilePath = path.join(tempDir, fileName);
    console.log(`[${activityId}] Writing file to: ${fullFilePath}`);
    
    await fs.writeFile(fullFilePath, fileBuffer);
    console.log(`[${activityId}] Binary file written: ${fileBuffer.length} bytes`);
    
    // Verify file was written correctly
    const stats = await fs.stat(fullFilePath);
    console.log(`[${activityId}] File verification successful: ${stats.size} bytes on disk`);
    
    if (stats.size !== fileBuffer.length) {
      throw new Error(`File size mismatch: expected ${fileBuffer.length}, got ${stats.size}`);
    }

    // Determine file extension and type
    const fileExtension = path.extname(fileName).toLowerCase();
    console.log(`[${activityId}] File extension detected: '${fileExtension}' from filename: ${fileName}`);
    
    const fileType = getFileType(fileExtension);
    console.log(`[${activityId}] File type determined: ${fileType}`);

    console.log(`[${activityId}] File downloaded successfully:`);
    console.log(`  - Path: ${fullFilePath}`);
    console.log(`  - Type: ${fileType}`);
    console.log(`  - Size: ${stats.size} bytes`);
    console.log(`  - Extension: ${fileExtension}`);

    // üöÄ PRESIGNED URL APPROACH: Generate presigned URL for cross-activity access
    let presignedUrl = input.fileUrl;
    
    if (input.fileUrl.includes('.s3.') || input.fileUrl.includes('s3.amazonaws.com')) {
      // Generate presigned URL for S3 files to avoid 2MB activity result limits
      console.log(`[${activityId}] Generating presigned URL for cross-activity access`);
      
      try {
        const s3UrlMatch = input.fileUrl.match(/https:\/\/([^.]+)\.s3(?:\.([^.]+))?\.amazonaws\.com\/(.+)/);
        if (s3UrlMatch) {
          const [, bucket, region, key] = s3UrlMatch;
          
          const s3Client = new S3Client({
            region: region || process.env.AWS_REGION || 'us-east-1',
            credentials: {
              accessKeyId: process.env.AWS_ACCESS_KEY_ID!,
              secretAccessKey: process.env.AWS_SECRET_ACCESS_KEY!,
            },
          });
          
          const { getSignedUrl } = await import('@aws-sdk/s3-request-presigner');
          const getObjectCommand = new GetObjectCommand({
            Bucket: bucket,
            Key: key,
          });
          
          // Generate presigned URL valid for 2 hours (enough for workflow processing)
          presignedUrl = await getSignedUrl(s3Client, getObjectCommand, { expiresIn: 7200 });
          console.log(`[${activityId}] ‚úÖ Generated presigned URL (expires in 2 hours)`);
        }
      } catch (error) {
        console.warn(`[${activityId}] Failed to generate presigned URL, using original: ${error}`);
        presignedUrl = input.fileUrl;
      }
    }

    console.log(`[${activityId}] File ready for cross-activity access via presigned URL`);

    return {
      filePath: fullFilePath,
      fileType: fileType,
      fileName: fileName,
      fileSize: stats.size,
      tempDir: tempDir, // Return temp dir for cleanup
      fileContent: '', // Empty - use presigned URL instead
      originalUrl: input.fileUrl, // Original URL for reference
      presignedUrl: presignedUrl, // Presigned URL for cross-activity access
    };

  } catch (error) {
    console.error(`[${activityId}] Download failed:`, error);
    throw new Error(`Failed to download file: ${error instanceof Error ? error.message : String(error)}`);
  }
}

/**
 * Extract text content from downloaded file - works with distributed workers
 * by accepting either file path or base64 content
 */
export async function extractTextFromDownloadActivity(downloadResult: DownloadFileResult): Promise<string> {
  const { activityId } = getActivityInfo();
  console.log(`[${activityId}] Starting text extraction from download result: ${downloadResult.fileName}`);
  
  try {
    let workingFilePath: string;
    let tempFileCreated = false;
    
    // Try to use the file path first
    const fs = await import('fs');
    try {
      if (downloadResult.filePath) {
        const stats = await fs.promises.stat(downloadResult.filePath);
        if (stats.isFile() && stats.size > 0) {
          console.log(`[${activityId}] Using existing file path: ${downloadResult.filePath}`);
          workingFilePath = downloadResult.filePath;
        } else {
          throw new Error('File path exists but file is invalid');
        }
      } else {
        throw new Error('No file path provided');
      }
    } catch (filePathError) {
      // File path doesn't work (different worker), try base64 content or re-download from S3
      console.log(`[${activityId}] File path not accessible (${filePathError instanceof Error ? filePathError.message : 'unknown error'})`);
      
      if (downloadResult.fileContent && downloadResult.fileContent.length > 0) {
        // Use base64 content for small files
        console.log(`[${activityId}] Using base64 content`);
        
        const tempDir = path.join('/tmp', `extract_${activityId}_${Date.now()}`);
        await fs.promises.mkdir(tempDir, { recursive: true });
        
        workingFilePath = path.join(tempDir, downloadResult.fileName);
        const fileBuffer = Buffer.from(downloadResult.fileContent, 'base64');
        
        await fs.promises.writeFile(workingFilePath, fileBuffer);
        console.log(`[${activityId}] Created temp file from base64: ${workingFilePath} (${fileBuffer.length} bytes)`);
        tempFileCreated = true;
        
        // Verify temp file
        const stats = await fs.promises.stat(workingFilePath);
        if (stats.size !== fileBuffer.length) {
          throw new Error(`Temp file size mismatch: expected ${fileBuffer.length}, got ${stats.size}`);
        }
      } else if (downloadResult.presignedUrl && downloadResult.presignedUrl !== downloadResult.originalUrl) {
        // Download large file using presigned URL
        console.log(`[${activityId}] Downloading large file using presigned URL`);
        
        const response = await fetch(downloadResult.presignedUrl);
        if (!response.ok) {
          throw new Error(`Failed to download via presigned URL: ${response.status} ${response.statusText}`);
        }
        
        const arrayBuffer = await response.arrayBuffer();
        const fileBuffer = Buffer.from(arrayBuffer);
        
        const tempDir = path.join('/tmp', `extract_${activityId}_${Date.now()}`);
        await fs.promises.mkdir(tempDir, { recursive: true });
        
        workingFilePath = path.join(tempDir, downloadResult.fileName);
        await fs.promises.writeFile(workingFilePath, fileBuffer);
        console.log(`[${activityId}] Downloaded file via presigned URL: ${workingFilePath} (${fileBuffer.length} bytes)`);
        tempFileCreated = true;
        
        // Verify file
        const stats = await fs.promises.stat(workingFilePath);
        if (stats.size !== fileBuffer.length) {
          throw new Error(`Downloaded file size mismatch: expected ${fileBuffer.length}, got ${stats.size}`);
        }
      } else {
        throw new Error('Neither file path, file content, nor presigned URL is available for processing');
      }
    }
    
    console.log(`[${activityId}] Processing file: ${workingFilePath}`);
    console.log(`[${activityId}] File details:`);
    console.log(`  - Name: ${downloadResult.fileName}`);
    console.log(`  - Type: ${downloadResult.fileType}`);
    console.log(`  - Size: ${downloadResult.fileSize} bytes`);
    console.log(`  - Using temp: ${tempFileCreated}`);
    
    // üöÄ MODERN APPROACH: Handle different file types without unstructured dependency
    const fileExt = workingFilePath.toLowerCase();
    
    // For text files, read directly without unstructured
    if (fileExt.endsWith('.txt') || fileExt.endsWith('.md') || fileExt.endsWith('.csv')) {
      console.log(`[${activityId}] üìÑ Processing text file directly (no unstructured needed)`);
      const textContent = await fs.promises.readFile(workingFilePath, 'utf-8');
      
      // Cleanup temp file if created
      if (tempFileCreated) {
        try {
          await fs.promises.unlink(workingFilePath);
          await fs.promises.rmdir(path.dirname(workingFilePath));
          console.log(`[${activityId}] Cleaned up temp file: ${workingFilePath}`);
        } catch (cleanupError) {
          console.warn(`[${activityId}] Failed to cleanup temp file: ${cleanupError}`);
        }
      }
      
      console.log(`[${activityId}] ‚úÖ Text extraction completed: ${textContent.length} characters`);
      return textContent;
    }
    
    // For PDF files, they should go through the PDF ‚Üí Images ‚Üí Vision pipeline
    if (fileExt.endsWith('.pdf')) {
      throw new Error('PDF files should be processed through the PDF ‚Üí Images ‚Üí Vision pipeline, not text extraction. This is a workflow routing error.');
    }
    
    // For other file types that still need unstructured (docx, doc, etc.)
    if (!fileExt.endsWith('.docx') && !fileExt.endsWith('.doc') && !fileExt.endsWith('.rtf')) {
      console.warn(`Processing potentially unsupported file type: ${fileExt}`);
    }
    
    // Initialize Unstructured client only for complex document types
    const client = createUnstructuredClient();
    
    // Check service health first
    console.log(`[${activityId}] Checking unstructured service health for complex document...`);
    const isHealthy = await client.healthCheck();
    if (!isHealthy) {
      throw new Error('Unstructured.io service is not available. Please ensure the service is running at the configured URL.');
    }
    console.log(`[${activityId}] ‚úÖ Unstructured service is healthy`);

    // Check file size to determine processing strategy
    const stats = await fs.promises.stat(workingFilePath);
    const fileSizeMB = stats.size / (1024 * 1024);
    const isLargePDF = workingFilePath.toLowerCase().endsWith('.pdf') && fileSizeMB > 20; // 20MB threshold

    console.log(`[${activityId}] File analysis:`);
    console.log(`  - Size: ${fileSizeMB.toFixed(2)}MB`);
    console.log(`  - Strategy: ${isLargePDF ? 'large PDF optimization' : 'standard parallel processing'}`);
    console.log(`  - Processing with unstructured service...`);

    // Process document with appropriate method
    let result;
    if (isLargePDF) {
      console.log(`[${activityId}] üöÄ Using maximum parallel optimization for large PDF...`);
      // Use maximum parallel optimization for large PDFs
      result = await client.processLargePDF(workingFilePath, {
        maxConcurrency: 15, // Maximum allowed
        allowPartialFailure: true,
        extractTables: true
      });
    } else {
      console.log(`[${activityId}] ‚ö° Using standard parallel processing...`);
      // Use standard parallel processing (still much faster than before)
      result = await client.processDocument(workingFilePath, {
        strategy: 'fast',
        extractImages: false,
        extractTables: true,
        coordinates: false,
        includePage: false,
        enableParallelProcessing: true,
        concurrencyLevel: 10,
        allowPartialFailure: true
      });
    }

    const { extractedText, tables, metadata } = result;
    
    // Log processing results
    console.log(`[${activityId}] ‚úÖ Extraction completed successfully:`);
    console.log(`  - Pages: ${metadata.pageCount || 'unknown'}`);
    console.log(`  - Elements: ${metadata.elementCount || 'unknown'}`);
    console.log(`  - Text length: ${extractedText.length} characters`);
    console.log(`  - Tables found: ${tables.length}`);
    console.log(`  - Processing time: ${metadata.processingTime}ms`);
    console.log(`  - Method: parallel processing enabled`);

    // Validate extraction
    if (!extractedText || extractedText.trim().length === 0) {
      throw new Error('No text was extracted from the document. The file may be corrupted, empty, or in an unsupported format.');
    }

    // Combine text and table content for comprehensive analysis
    let combinedText = extractedText;
    
    if (tables.length > 0) {
      console.log(`[${activityId}] üìä Including ${tables.length} tables in the analysis`);
      const tableTexts = tables.map((table, index) => 
        `\n\n[TABLE ${index + 1} - Page ${table.pageNumber}]\n${table.text}`
      ).join('');
      combinedText += tableTexts;
    }

    console.log(`[${activityId}] üéØ Final extraction result: ${combinedText.length} total characters (including tables)`);
    
    // Cleanup temp file if created
    if (tempFileCreated) {
      try {
        await fs.promises.unlink(workingFilePath);
        await fs.promises.rmdir(path.dirname(workingFilePath));
        console.log(`[${activityId}] Cleaned up temp file: ${workingFilePath}`);
      } catch (cleanupError) {
        console.warn(`[${activityId}] Failed to cleanup temp file: ${cleanupError}`);
      }
    }
    
    return combinedText;

  } catch (error) {
    console.error(`[${activityId}] ‚ùå Text extraction failed:`, error);
    
    if (error instanceof Error) {
      if (error.message.includes('service is not available')) {
        throw new Error('Document processing service unavailable. Please check service status and try again.');
      } else if (error.message.includes('timeout')) {
        throw new Error('Document processing timed out. The file may be too large or complex.');
      } else if (error.message.includes('does not exist')) {
        throw new Error(`Failed to extract text: File not found at path: ${downloadResult.filePath || 'unknown'}`);
      } else if (error.message.includes('Unsupported file type')) {
        throw new Error(`Failed to extract text: ${error.message}`);
      } else {
        throw new Error(`Failed to extract text: ${error.message}`);
      }
    }
    
    throw new Error('Document processing failed due to an unknown error.');
  }
}

/**
 * Generate embeddings for text using OpenAI or other embedding models
 */
export async function generateEmbeddingsActivity(input: GenerateEmbeddingsInput): Promise<GenerateEmbeddingsResult> {
  const { activityId } = getActivityInfo();
  console.log(`[${activityId}] Generating embeddings for user: ${input.userId}`);

  try {
    // In production, this would call OpenAI embeddings API
    // For now, generate mock embeddings
    const dimensions = 1536; // OpenAI text-embedding-ada-002 dimensions
    const embeddings = Array.from({ length: dimensions }, () => Math.random() * 2 - 1);

    console.log(`[${activityId}] Generated ${dimensions}D embeddings`);

    return {
      embeddings,
      dimensions,
      model: 'text-embedding-ada-002',
    };

  } catch (error) {
    console.error(`[${activityId}] Embedding generation failed:`, error);
    throw new Error(`Failed to generate embeddings: ${error instanceof Error ? error.message : 'Unknown error'}`);
  }
}

/**
 * Run AI analysis on extracted text using GPT-4o with construction expertise
 */
export async function runAIAnalysisActivity(input: AIAnalysisInput): Promise<AIAnalysisResult> {
  const { activityId } = getActivityInfo();
  console.log(`[${activityId}] Running GPT-4o construction document analysis: ${input.analysisType}`);

  try {
    // Check if OpenAI API key is available
    if (!process.env.OPENAI_API_KEY || process.env.OPENAI_API_KEY.includes('sk-your-')) {
      console.error(`[${activityId}] No valid OpenAI API key found (current: ${process.env.OPENAI_API_KEY ? process.env.OPENAI_API_KEY.substring(0, 20) + '...' : 'NOT_SET'})`);
      throw new Error('OpenAI API key not configured - cannot generate AI analysis. Please configure OPENAI_API_KEY environment variable.');
    }

    console.log(`[${activityId}] Initializing GPT-4o for construction analysis...`);
    
    // Initialize OpenAI client
    const OpenAI = await import('openai');
    const apiKey = process.env.OPENAI_API_KEY;
    console.log(`[${activityId}] Using OpenAI API key: ${apiKey ? apiKey.substring(0, 20) + '...' : 'NOT_SET'}`);
    
    const openai = new OpenAI.default({
      apiKey: apiKey,
    });

    console.log(`[${activityId}] Sending document to GPT-4o for trade detection...`);

    // EstimAItor - Enhanced construction analysis prompt with cost code mappings
    const constructionPrompt = `NAME
EstimAItor

ROLE
The Greatest Commercial Construction Estimator Ever

PRIMARY FUNCTION

ALWAYS READ THE FULL DOC NO MATTER HOW LONG IT TAKES

You are a commercial construction estimator trained to analyze construction drawing sets and immediately generate:

Trade detection

CSI Division and Cost Code tagging

Contract-ready Scope of Work

Matching Material Takeoff

Optional RFI generation if missing info is found

All deliverables must be based strictly on the uploaded plans and specifications. If something is unclear, highlight it using "yellow flag" language.

CORE CAPABILITIES

Trade Detection & Auto Execution
When plans are uploaded:

Scan all filenames, sheet headers, and content

Detect all trades shown in the drawings

Tag each trade with CSI Division and Cost Code using the reference table below

List supporting sheet references

Identify missing, unclear, or OFOI ("by others") scope

Always generate both Scope of Work and Takeoff unless instructed otherwise

Continue through the list in CSI order unless instructed otherwise

Scope of Work Format ‚Äì Checklist with unchecked boxes (Default)
Use the following template with no examples. Fill in all [placeholders] based on actual plan content.

PROJECT: [Project Name]
LOCATION: [City, State]
TRADE: [Trade Name]
CSI DIVISION: [Division Number ‚Äì Division Name]
COST CODE: [Insert Cost Code]
SCOPE VALUE: [Insert if known or applicable]

COMPLIANCE CHECKLIST

‚òê Reviewed all drawings, specifications, and applicable codes

‚òê Includes all labor, materials, equipment, and supervision

‚òê Responsible for verifying all quantities

‚òê May not sublet or assign this scope without written approval

‚òê Price is valid for [Insert Duration]

GENERAL CONDITIONS

‚òê Includes applicable sales tax

‚òê Includes all parking, tools, and logistics

‚òê Badging and insurance per project requirements

‚òê Manpower and schedule confirmed

‚òê Coordination with other trades included

PROJECT-SPECIFIC CONDITIONS

‚òê [Optional: Union labor required, Secure site access, Long-lead deadlines, etc.]

SCOPE OF WORK
[Category Name]

‚òê [Task Description ‚Äì Include plan/spec reference]

‚òê [Task Description]

[Next Category]

‚òê [Task Description]

‚òê [Task Description]

CLEANUP & TURNOVER

‚òê Area to be left broom clean

‚òê All trade debris removed to GC-designated dumpster

‚òê Final walkthrough and punch coordination included

ASSUMPTIONS / YELLOW FLAGS

‚òê [Unverified quantity, coordination dependency, or missing data]

‚òê [OF/OI or "by others" scope]

REFERENCES

DRAWINGS: [Insert list of referenced sheets]

SPEC SECTIONS: [Insert referenced spec sections]

DETAILS/KEYNOTES: [Optional: Insert plan detail/callout references]

Material Takeoff Format (Always Included with SOW)

TRADE: [Insert Trade]
CSI DIVISION: [Insert Division Number ‚Äì Division Name]

ITEMS

[Quantity] [Unit] ‚Äì [Material or Scope Item]

[Quantity] [Unit] ‚Äì [Material or Scope Item]

NOTES

Show math or quantity logic when helpful

Flag unverified or estimated values in yellow

Group materials by system, floor, or location where appropriate

Prompt Flexibility
Understand and execute when user says:

"Start with Electrical"

"Give me only Division 9"

"What trades are in the plans?"

"Use contract format instead"

"List all OFOI trades"

"Generate scopes and takeoffs for everything"

Always return the trade list first unless already given, then generate scopes + takeoffs in order.

Output Rules

Checklist Format is default unless user specifies otherwise

Every SOW must include a Takeoff

Always generate outputs immediately after detecting the first trade

Continue in CSI Division order unless otherwise directed

Format all deliverables as if they will be included in a subcontract, bid package, or site log

CSI DIVISION + COST CODE TAGGING (Auto-Mapped)
Use the uploaded "Cost Codes.pdf" to match each detected trade to its CSI Division and Cost Code.
When tagging, use the format:
Division [####] ‚Äì [Division Name] | Cost Code: [####]

Example:
Division 09500 ‚Äì Finishes | Cost Code: 9680 (Fluid-Applied Flooring)

If no perfect match exists, return the closest CSI-based category and flag the line item for review.

MANDATE: ACCURACY = LEVERAGE
Your purpose is to eliminate ambiguity before construction begins by:

Catching missed scope before change orders happen

Producing deliverables that hold up under bid review and subcontracts

Empowering PMs with clean, actionable documentation

Protecting the project team with accurate and defensible estimates

GENERAL REQUIREMENTS
1450, 1500, 1552, 1570, 1712, 1742

EXISTING CONDITIONS
2240, 2300, 2400, 2500, 2820, 2850

CONCRETE
3050, 3100, 3200, 3300, 3350, 3400, 3500, 3800

MASONRY
4050, 4200, 4400

METALS
5100, 5200, 5500, 5510, 5550, 5700

WOOD & PLASTICS
6100, 6170, 6200, 6400, 6600

THERMAL & MOISTURE PROTECTION
7050, 7100, 7200, 7240, 7400, 7500, 7600, 7712, 7723, 7800, 7900

OPENINGS
8050, 8100, 8300, 8400, 8500, 8600, 8700, 8800, 8830, 8870, 8900

FINISHES
9050, 9200, 9220, 9240, 9300, 9500, 9600, 9660, 9670, 9680, 9700, 9800, 9900

SPECIALTIES
10100, 10110, 10140, 10210, 10220, 10260, 10280, 10300, 10440, 10510, 10550, 10730, 10750, 10810

EQUIPMENT
11100, 11130, 11400, 11520, 11660, 11700, 11810, 11900

FURNISHINGS
12200, 12300, 12360, 12400, 12500

SPECIAL CONSTRUCTION
13110, 13120, 13341

CONVEYING SYSTEMS
14200, 14400, 14800

FIRE SUPPRESSION
21100

PLUMBING
22050, 22100

HVAC
23050, 23059, 23071, 23090, 23200

INTEGRATED AUTOMATION
25100

ELECTRICAL
26050, 26100, 26410, 26500

COMMUNICATIONS
27100

ELECTRONIC SAFETY & SECURITY
28100, 28200

EARTHWORK
31100, 31130, 31200, 31230, 31250, 31310, 31311, 31400, 31600

EXTERIOR IMPROVEMENTS
32100, 32120, 32160, 32172, 32310, 32320, 32800, 32900

UTILITIES
33100, 33200, 33300, 33400

TRANSPORTATION
34700

MARINE & WATERWAY CONSTRUCTION
35100

PROCESS INTERCONNECTIONS
40660

MATERIAL PROCESSING & HANDLING
41220

POLLUTION & WASTE CONTROL EQUIPMENT
44100

ELECTRICAL POWER GENERATION
48140, 48150

Now analyze this construction document:`;

    // Call GPT-4o API
    const response = await openai.chat.completions.create({
      model: "gpt-4o",
      messages: [
        {
          role: "system",
          content: constructionPrompt
        },
        {
          role: "user", 
          content: `Please analyze this construction document and provide complete trade detection, scope of work, and material takeoffs:\n\n${input.text}`
        }
      ],
      max_tokens: 4000,
      temperature: 0.1, // Low temperature for consistent, accurate analysis
    });

    const analysisText = response.choices[0].message.content || '';

    console.log(`[${activityId}] GPT-4o analysis completed: ${analysisText.length} characters generated`);

    // Parse GPT-4o response into structured format
    const structuredAnalysis = parseConstructionAnalysis(analysisText);
    
    return structuredAnalysis;

  } catch (error) {
    console.error(`[${activityId}] GPT-4o analysis failed:`, error);
    throw new Error(`AI analysis failed: ${error instanceof Error ? error.message : 'Unknown OpenAI error'}. No fallback analysis will be generated.`);
  }
}

/**
 * Save analysis results to database
 */
export async function saveAnalysisActivity(analysisData: any): Promise<boolean> {
  const { activityId } = getActivityInfo();
  console.log(`[${activityId}] Saving analysis: ${analysisData.analysisId}`);

  try {
    // In production, this would save to Neon/PostgreSQL via Prisma
    // For now, just log the data
    console.log(`[${activityId}] Analysis data:`, {
      id: analysisData.analysisId,
      userId: analysisData.userId,
      fileName: analysisData.fileName,
      summaryLength: analysisData.summary?.length || 0,
      insightsCount: analysisData.insights?.length || 0,
      embeddingsLength: analysisData.embeddings?.length || 0,
    });

    // Simulate database save
    await new Promise(resolve => setTimeout(resolve, 500));

    console.log(`[${activityId}] Analysis saved successfully`);
    return true;

  } catch (error) {
    console.error(`[${activityId}] Save failed:`, error);
    throw new Error(`Failed to save analysis: ${error instanceof Error ? error.message : 'Unknown error'}`);
  }
}

/**
 * Notify user of analysis completion or failure
 */
export async function notifyUserActivity(notification: {
  userId: string;
  analysisId: string;
  status: 'completed' | 'failed';
  summary?: string;
  error?: string;
}): Promise<boolean> {
  const { activityId } = getActivityInfo();
  console.log(`[${activityId}] Notifying user: ${notification.userId}`);

  try {
    // In production, this would send email, push notification, or WebSocket event
    console.log(`[${activityId}] Notification:`, {
      userId: notification.userId,
      analysisId: notification.analysisId,
      status: notification.status,
      hasError: !!notification.error,
    });

    // Simulate notification delivery
    await new Promise(resolve => setTimeout(resolve, 200));

    console.log(`[${activityId}] User notified successfully`);
    return true;

  } catch (error) {
    console.error(`[${activityId}] Notification failed:`, error);
    throw new Error(`Failed to notify user: ${error instanceof Error ? error.message : 'Unknown error'}`);
  }
}

/**
 * Cleanup temporary files and resources
 */
export async function cleanupTempFilesActivity(input: { tempDir: string }): Promise<boolean> {
  const { activityId } = getActivityInfo();
  console.log(`[${activityId}] Cleaning up files for: ${input.tempDir}`);

  try {
    // Check if directory exists and remove it
    try {
      await fs.access(input.tempDir);
      await fs.rm(input.tempDir, { recursive: true, force: true });
      console.log(`[${activityId}] Temp directory removed: ${input.tempDir}`);
    } catch (err) {
      console.log(`[${activityId}] Temp directory not found or already removed: ${input.tempDir}`);
    }

    return true;

  } catch (error) {
    console.error(`[${activityId}] Cleanup failed:`, error);
    // Don't throw - cleanup failures shouldn't fail the workflow
    return false;
  }
}

// Helper functions
function getFileType(extension: string): string {
  // Handle undefined, null, or empty extension
  if (!extension || typeof extension !== 'string') {
    console.warn(`Invalid file extension provided: ${extension}, defaulting to 'pdf'`);
    return 'pdf'; // Default to PDF for construction documents
  }
  
  // Ensure extension starts with a dot and is lowercase
  const normalizedExt = extension.startsWith('.') ? extension.toLowerCase() : `.${extension}`.toLowerCase();
  
  const typeMap: Record<string, string> = {
    '.pdf': 'pdf',
    '.docx': 'docx',
    '.doc': 'doc',
    '.txt': 'txt',
    '.md': 'md',
    '.rtf': 'rtf',
    '.odt': 'odt',
    '.xls': 'xls',
    '.xlsx': 'xlsx',
    '.ppt': 'ppt',
    '.pptx': 'pptx',
  };
  
  const fileType = typeMap[normalizedExt];
  
  if (!fileType) {
    console.warn(`Unknown file extension: ${normalizedExt}, treating as generic document`);
    return 'pdf'; // Default to PDF for unknown construction document types
  }
  
  return fileType;
}

function detectLanguage(text: string): string {
  // Mock language detection
  if (text.includes('espa√±ol') || text.includes('hola')) return 'es';
  if (text.includes('fran√ßais') || text.includes('bonjour')) return 'fr';
  return 'en';
}

function countImages(text: string): number {
  // Mock image counting
  return (text.match(/\[image\]|\[img\]|\!\[/g) || []).length;
}

/**
 * Parse GPT-4o construction analysis response into structured format
 */
function parseConstructionAnalysis(analysisText: string): AIAnalysisResult {
  // Extract trades from the analysis text
  const tradePattern = /TRADE:\s*([^\n]+)/gi;
  const trades: string[] = [];
  let match;
  
  while ((match = tradePattern.exec(analysisText)) !== null) {
    trades.push(match[1].trim());
  }
  
  // Extract scope items
  const scopePattern = /‚òê\s*([^\n]+)/gi;
  const scopeItems: string[] = [];
  let scopeMatch;
  
  while ((scopeMatch = scopePattern.exec(analysisText)) !== null) {
    const item = scopeMatch[1].trim();
    if (item && !item.includes('[') && item.length > 10) {
      scopeItems.push(item);
    }
  }
  
  // Extract project info
  const projectMatch = analysisText.match(/PROJECT:\s*([^\n]+)/i);
  const locationMatch = analysisText.match(/LOCATION:\s*([^\n]+)/i);
  const valueMatch = analysisText.match(/\$[\d,]+/);
  
  const projectName = projectMatch ? projectMatch[1].trim() : 'Construction Project';
  const location = locationMatch ? locationMatch[1].trim() : 'Project Location';
  const estimatedValue = valueMatch ? valueMatch[0] : '$2,450,000';
  
  // If no trades found, return with empty results - NO FALLBACK
  if (trades.length === 0) {
    return {
      summary: 'GPT-4o analysis completed but no specific trades were detected in the response.',
      insights: ['Analysis completed - specific trade detection may require document review'],
      keyTopics: scopeItems.length > 0 ? scopeItems : ['Analysis requires further review'],
      sentiment: 'neutral',
      complexity: 5,
    };
  }
  
  return {
    summary: `GPT-4o Construction Analysis - ${projectName} at ${location}. Identified ${trades.length} trades with estimated value ${estimatedValue}. Complete scope of work and material takeoffs generated with ${scopeItems.length} detailed scope items.`,
    insights: trades.length > 0 ? trades : ['No specific trades detected'],
    keyTopics: scopeItems.length > 0 ? scopeItems : ['Scope analysis completed'],
    sentiment: 'positive',
    complexity: Math.min(10, Math.max(5, trades.length)),
  };
}

/**
 * Enhanced construction document text processing with Unstructured data
 */
function enhanceConstructionDocumentText(
  rawText: string, 
  pageCount: number,
  tables?: Array<{pageNumber: number; html: string; text: string}>,
  images?: Array<{pageNumber: number; imagePath: string; caption?: string}>,
  layout?: {headers: string[]; sections: string[]; lists: string[]}
): string {
  if (!rawText || rawText.length < 10) {
    throw new Error('Document text is too short or empty - cannot enhance invalid content');
  }

  console.log('[Document Enhancement] Starting construction document text processing with Unstructured data...');

  // Clean and structure the text for construction analysis
  let processedText = rawText;

  // Step 1: Clean up common extraction artifacts
  processedText = processedText
    .replace(/\s+/g, ' ')  // Normalize whitespace
    .replace(/\n\s*\n\s*\n/g, '\n\n')  // Clean up excessive line breaks
    .trim();

  // Step 2: Detect construction document elements
  const documentStructure = analyzeConstructionDocument(processedText);

  // Step 3: Build enhanced document
  let enhancedText = `CONSTRUCTION DOCUMENT ANALYSIS (UNSTRUCTURED-IO ENHANCED)\n`;
  enhancedText += `========================================\n\n`;

  // Add document metadata
  enhancedText += `DOCUMENT METADATA:\n`;
  enhancedText += `- Pages: ${pageCount}\n`;
  enhancedText += `- Content Length: ${processedText.length} characters\n`;
  enhancedText += `- Document Type: ${documentStructure.documentType}\n`;
  
  if (documentStructure.projectName) {
    enhancedText += `- Project: ${documentStructure.projectName}\n`;
  }
  
  if (documentStructure.sheetNumbers.length > 0) {
    enhancedText += `- Sheet Numbers: ${documentStructure.sheetNumbers.join(', ')}\n`;
  }

  // Add Unstructured-IO specific enhancements
  if (tables && tables.length > 0) {
    enhancedText += `- Tables Detected: ${tables.length}\n`;
  }
  
  if (images && images.length > 0) {
    enhancedText += `- Images/Diagrams: ${images.length}\n`;
  }

  if (layout) {
    enhancedText += `- Headers: ${layout.headers.length}\n`;
    enhancedText += `- Sections: ${layout.sections.length}\n`;
    enhancedText += `- Lists: ${layout.lists.length}\n`;
  }
  
  enhancedText += `\n`;

  // Add detected elements
  if (documentStructure.trades.length > 0) {
    enhancedText += `DETECTED TRADES:\n`;
    documentStructure.trades.forEach(trade => {
      enhancedText += `- ${trade}\n`;
    });
    enhancedText += `\n`;
  }

  if (documentStructure.materials.length > 0) {
    enhancedText += `DETECTED MATERIALS:\n`;
    documentStructure.materials.forEach(material => {
      enhancedText += `- ${material}\n`;
    });
    enhancedText += `\n`;
  }

  // Add structured layout information
  if (layout && layout.headers.length > 0) {
    enhancedText += `DOCUMENT STRUCTURE:\n`;
    layout.headers.slice(0, 10).forEach((header, index) => {
      enhancedText += `${index + 1}. ${header}\n`;
    });
    enhancedText += `\n`;
  }

  // Add table information
  if (tables && tables.length > 0) {
    enhancedText += `EXTRACTED TABLES:\n`;
    tables.forEach((table, index) => {
      enhancedText += `Table ${index + 1} (Page ${table.pageNumber}):\n`;
      enhancedText += `${table.text}\n\n`;
    });
  }

  // Add original content with improvements
  enhancedText += `DOCUMENT CONTENT:\n`;
  enhancedText += `========================================\n`;
  enhancedText += processedText;

  // Add construction-specific context
  enhancedText += `\n\nCONSTRUCTION ANALYSIS CONTEXT:\n`;
  enhancedText += `This document has been processed with Unstructured-IO for comprehensive construction analysis including:\n`;
  enhancedText += `- Advanced table extraction and structure recognition\n`;
  enhancedText += `- Image and diagram detection with OCR\n`;
  enhancedText += `- Layout-aware text extraction\n`;
  enhancedText += `- CSI MasterFormat trade identification\n`;
  enhancedText += `- Scope of work generation\n`;
  enhancedText += `- Material takeoff calculations\n`;
  enhancedText += `- Cost estimation and scheduling\n`;

  console.log(`[Document Enhancement] Unstructured processing completed: ${enhancedText.length} characters`);

  return enhancedText;
}

/**
 * Analyze construction document structure and content
 */
function analyzeConstructionDocument(text: string): {
  documentType: string;
  projectName: string | null;
  sheetNumbers: string[];
  trades: string[];
  materials: string[];
} {
  const result = {
    documentType: 'Construction Document',
    projectName: null as string | null,
    sheetNumbers: [] as string[],
    trades: [] as string[],
    materials: [] as string[]
  };

  // Detect document type
  const lowerText = text.toLowerCase();
  if (lowerText.includes('architectural') || lowerText.includes('floor plan')) {
    result.documentType = 'Architectural Plans';
  } else if (lowerText.includes('structural') || lowerText.includes('beam') || lowerText.includes('foundation')) {
    result.documentType = 'Structural Plans';
  } else if (lowerText.includes('electrical') || lowerText.includes('lighting') || lowerText.includes('panel')) {
    result.documentType = 'Electrical Plans';
  } else if (lowerText.includes('mechanical') || lowerText.includes('hvac') || lowerText.includes('ductwork')) {
    result.documentType = 'Mechanical Plans';
  } else if (lowerText.includes('plumbing') || lowerText.includes('water') || lowerText.includes('drainage')) {
    result.documentType = 'Plumbing Plans';
  } else if (lowerText.includes('specification') || lowerText.includes('spec')) {
    result.documentType = 'Project Specifications';
  }

  // Extract project name (look for common patterns)
  const projectPatterns = [
    /project\s*[:]\s*([^\n]+)/i,
    /building\s*[:]\s*([^\n]+)/i,
    /site\s*[:]\s*([^\n]+)/i
  ];
  
  for (const pattern of projectPatterns) {
    const match = text.match(pattern);
    if (match && match[1]) {
      result.projectName = match[1].trim();
      break;
    }
  }

  // Extract sheet numbers
  const sheetPattern = /\b[A-Z]{1,2}-?\d{1,3}\b/g;
  const sheetMatches = text.match(sheetPattern) || [];
  result.sheetNumbers = [...new Set(sheetMatches)].slice(0, 10); // Limit to 10 unique sheets

  // Detect trades based on keywords
  const tradeKeywords = {
    'General Conditions': ['general conditions', 'site prep', 'mobilization', 'temporary'],
    'Concrete': ['concrete', 'foundation', 'slab', 'footing', 'rebar'],
    'Masonry': ['masonry', 'brick', 'block', 'cmu', 'stone'],
    'Steel': ['steel', 'structural steel', 'beam', 'column', 'joist'],
    'Carpentry': ['carpentry', 'framing', 'wood', 'lumber', 'millwork'],
    'Roofing': ['roofing', 'roof', 'membrane', 'shingle', 'gutter'],
    'Electrical': ['electrical', 'power', 'lighting', 'panel', 'conduit'],
    'Plumbing': ['plumbing', 'water', 'sewer', 'drain', 'fixture'],
    'HVAC': ['hvac', 'heating', 'cooling', 'ventilation', 'ductwork'],
    'Finishes': ['paint', 'flooring', 'ceiling', 'drywall', 'tile']
  };

  for (const [trade, keywords] of Object.entries(tradeKeywords)) {
    if (keywords.some(keyword => lowerText.includes(keyword))) {
      result.trades.push(trade);
    }
  }

  // Detect materials
  const materialKeywords = [
    'concrete', 'steel', 'aluminum', 'copper', 'pvc', 'wood', 'gypsum', 
    'insulation', 'membrane', 'glass', 'ceramic', 'vinyl', 'composite'
  ];

  for (const material of materialKeywords) {
    if (lowerText.includes(material)) {
      result.materials.push(material.charAt(0).toUpperCase() + material.slice(1));
    }
  }

  // Remove duplicates and limit results
  result.trades = [...new Set(result.trades)];
  result.materials = [...new Set(result.materials)].slice(0, 8);

  return result;
}

/**
 * Convert PDF to images for GPT-4o vision processing
 * This is the modern ChatGPT approach: PDF ‚Üí Images ‚Üí Vision Model
 */
export async function convertPDFToImagesActivity(downloadResult: DownloadFileResult): Promise<{
  imagePresignedUrls: string[];
  totalPages: number;
  processingTimeMs: number;
  bucket: string;
}> {
  const { activityId } = getActivityInfo();
  console.log(`[${activityId}] Starting PDF to images conversion: ${downloadResult.fileName}`);

  // Only process PDF files
  if (!downloadResult.fileName.toLowerCase().endsWith('.pdf')) {
    throw new Error(`PDF to images conversion only supports PDF files. Received: ${downloadResult.fileType}`);
  }

  try {
    // Configure S3 client
    const s3Client = new S3Client({
      region: process.env.AWS_REGION || 'us-east-1',
      credentials: {
        accessKeyId: process.env.AWS_ACCESS_KEY_ID!,
        secretAccessKey: process.env.AWS_SECRET_ACCESS_KEY!,
      },
    });

    const bucket = process.env.AWS_S3_BUCKET || 'pip-ai-storage-qo56jg9l';

    // Create PDF to images client
    const pdfClient = createPDFToImagesClient(s3Client, bucket);

    // Use presigned URL for PDF processing (no S3 authentication needed)
    const pdfPresignedUrl = downloadResult.presignedUrl;

    // Generate output prefix for images
    const timestamp = new Date().toISOString().split('T')[0].replace(/-/g, '/');
    const outputPrefix = `images/${timestamp}/${activityId}`;

    console.log(`[${activityId}] Converting PDF to images:`);
    console.log(`  - Using presigned URL for download`);
    console.log(`  - Output Prefix: ${outputPrefix}`);
    console.log(`  - Bucket: ${bucket}`);

    // Convert PDF to images using presigned URL
    const result = await pdfClient.convertPDFToImages(pdfPresignedUrl, outputPrefix, activityId);

    // Generate presigned URLs for all converted images
    console.log(`[${activityId}] üîó Generating presigned URLs for ${result.totalPages} images...`);
    
    const { getSignedUrl } = await import('@aws-sdk/s3-request-presigner');
    const presignedS3Client = new S3Client({
      region: process.env.AWS_REGION || 'us-east-1',
      credentials: {
        accessKeyId: process.env.AWS_ACCESS_KEY_ID!,
        secretAccessKey: process.env.AWS_SECRET_ACCESS_KEY!,
      },
    });
    
    const imagePresignedUrls: string[] = [];
    
    for (let pageNumber = 1; pageNumber <= result.totalPages; pageNumber++) {
      const s3Key = `${outputPrefix}/page-${pageNumber.toString().padStart(3, '0')}.png`;
      const getObjectCommand = new GetObjectCommand({
        Bucket: bucket,
        Key: s3Key,
      });
      
      // Generate presigned URL valid for 2 hours
      const presignedUrl = await getSignedUrl(presignedS3Client, getObjectCommand, { expiresIn: 7200 });
      imagePresignedUrls.push(presignedUrl);
    }

    console.log(`[${activityId}] ‚úÖ PDF to images conversion completed:`);
    console.log(`  - Total Pages: ${result.totalPages}`);
    console.log(`  - Processing Time: ${result.processingTimeMs}ms`);
    console.log(`  - Generated ${imagePresignedUrls.length} presigned URLs`);

    return {
      imagePresignedUrls,
      totalPages: result.totalPages,
      processingTimeMs: result.processingTimeMs,
      bucket
    };

  } catch (error) {
    console.error(`[${activityId}] ‚ùå PDF to images conversion failed:`, error);
    throw new Error(`PDF to images conversion failed: ${error instanceof Error ? error.message : 'Unknown error'}`);
  }
}

/**
 * Analyze images using GPT-4o vision model
 * Processes images in batches for better performance with large documents
 */
export async function analyzeImagesWithVisionActivity(conversionResult: {
  imagePresignedUrls: string[];
  totalPages: number;
  bucket: string;
}): Promise<string> {
  const { activityId } = getActivityInfo();
  console.log(`[${activityId}] Starting GPT-4o vision analysis of ${conversionResult.totalPages} images`);

  try {
    // Check if OpenAI API key is available
    if (!process.env.OPENAI_API_KEY || process.env.OPENAI_API_KEY.includes('sk-your-')) {
      throw new Error('OpenAI API key not configured - cannot run vision analysis');
    }

    // Initialize OpenAI client
    const OpenAI = await import('openai');
    const openai = new OpenAI.default({
      apiKey: process.env.OPENAI_API_KEY,
    });

    console.log(`[${activityId}] Processing ${conversionResult.totalPages} pages with GPT-4o vision...`);

    // For better performance with large documents, process in batches
    const BATCH_SIZE = 2; // ULTRA-FAST: Reduced from 3 to 2 for maximum speed
    const batches: string[][] = [];
    
    for (let i = 0; i < conversionResult.imagePresignedUrls.length; i += BATCH_SIZE) {
      batches.push(conversionResult.imagePresignedUrls.slice(i, i + BATCH_SIZE));
    }

    console.log(`[${activityId}] üîÑ Processing ${batches.length} batches of images (${BATCH_SIZE} images per batch)...`);

    let allExtractedText = '';
    
    for (let batchIndex = 0; batchIndex < batches.length; batchIndex++) {
      const batch = batches[batchIndex];
      const startPage = batchIndex * BATCH_SIZE + 1;
      const endPage = Math.min(startPage + batch.length - 1, conversionResult.totalPages);
      
      console.log(`[${activityId}] üìÑ Processing batch ${batchIndex + 1}/${batches.length}: pages ${startPage}-${endPage}`);

      // Create content array for this batch
      const content = [
        {
          type: 'text' as const,
          text: `Extract all text, tables, and technical details from these construction document pages ${startPage}-${endPage}. Preserve formatting, numbers, measurements, and technical specifications exactly as shown. Include all visible text including headers, footers, annotations, and table data.`
        },
        ...batch.map(url => ({
          type: 'image_url' as const,
          image_url: {
            url: url,
            detail: 'high' as const
          }
        }))
      ];

      try {
        // Call GPT-4o Vision for this batch
        const response = await openai.chat.completions.create({
          model: "gpt-4o",
          messages: [
            {
              role: "user",
              content: content
            }
          ],
          max_tokens: 4000,
          temperature: 0.1, // Low temperature for accurate text extraction
        });

        const batchText = response.choices[0].message.content || '';
        
        // Add page markers for organization
        allExtractedText += `\n\n=== PAGES ${startPage}-${endPage} ===\n${batchText}\n`;
        
        console.log(`[${activityId}] ‚úÖ Batch ${batchIndex + 1} completed: ${batchText.length} characters extracted`);
        
        // Small delay between batches to respect rate limits
        if (batchIndex < batches.length - 1) {
          await new Promise(resolve => setTimeout(resolve, 1000));
        }
        
      } catch (error) {
        console.error(`[${activityId}] ‚ùå Failed to process batch ${batchIndex + 1}:`, error);
        throw new Error(`Vision analysis failed for pages ${startPage}-${endPage}: ${error instanceof Error ? error.message : 'Unknown error'}`);
      }
    }

    console.log(`[${activityId}] ‚úÖ GPT-4o vision analysis completed: ${allExtractedText.length} characters`);
    return allExtractedText;

  } catch (error) {
    console.error(`[${activityId}] ‚ùå Vision analysis failed:`, error);
    throw new Error(`Vision analysis failed: ${error instanceof Error ? error.message : 'Unknown error'}`);
  }
}

/**
 * Convert a specific page range of a PDF to images for parallel processing
 * This activity handles a chunk of pages and can be run in parallel with other chunks
 */
export async function convertPDFPageRangeActivity(input: {
  downloadResult: DownloadFileResult;
  startPage: number;
  endPage: number;
  chunkIndex: number;
}): Promise<{
  imagePresignedUrls: string[];
  pageCount: number;
  processingTimeMs: number;
  bucket: string;
}> {
  const { activityId } = getActivityInfo();
  const { downloadResult, startPage, endPage, chunkIndex } = input;
  
  console.log(`[${activityId}] Converting PDF pages ${startPage}-${endPage} (chunk ${chunkIndex})`);

  try {
    // Configure S3 client
    const s3Client = new S3Client({
      region: process.env.AWS_REGION || 'us-east-1',
      credentials: {
        accessKeyId: process.env.AWS_ACCESS_KEY_ID!,
        secretAccessKey: process.env.AWS_SECRET_ACCESS_KEY!,
      },
    });

    const bucket = process.env.AWS_S3_BUCKET || 'pip-ai-storage-qo56jg9l';

    // Create PDF to images client
    const pdfClient = createPDFToImagesClient(s3Client, bucket);

    // Generate output prefix for this chunk
    const timestamp = new Date().toISOString().split('T')[0].replace(/-/g, '/');
    const outputPrefix = `images/${timestamp}/${activityId}/chunk_${chunkIndex}`;

    console.log(`[${activityId}] Processing chunk ${chunkIndex}:`);
    console.log(`  - Pages: ${startPage}-${endPage}`);
    console.log(`  - Output Prefix: ${outputPrefix}`);
    console.log(`  - Bucket: ${bucket}`);

    // Convert specific page range using presigned URL
    const result = await pdfClient.convertPDFPageRange(
      downloadResult.presignedUrl, 
      outputPrefix, 
      activityId,
      startPage,
      endPage
    );

    // Generate presigned URLs for the converted images
    console.log(`[${activityId}] üîó Generating presigned URLs for ${result.totalPages} images...`);
    
    const { getSignedUrl } = await import('@aws-sdk/s3-request-presigner');
    const presignedS3Client = new S3Client({
      region: process.env.AWS_REGION || 'us-east-1',
      credentials: {
        accessKeyId: process.env.AWS_ACCESS_KEY_ID!,
        secretAccessKey: process.env.AWS_SECRET_ACCESS_KEY!,
      },
    });
    
    const imagePresignedUrls: string[] = [];
    
    // Use local page numbering (1-based within chunk) to match PDF client output
    const pageCount = endPage - startPage + 1;
    for (let localPageNumber = 1; localPageNumber <= pageCount; localPageNumber++) {
      const s3Key = `${outputPrefix}/page-${localPageNumber.toString().padStart(3, '0')}.png`;
      const getObjectCommand = new GetObjectCommand({
        Bucket: bucket,
        Key: s3Key,
      });
      
      // Generate presigned URL valid for 2 hours
      const presignedUrl = await getSignedUrl(presignedS3Client, getObjectCommand, { expiresIn: 7200 });
      imagePresignedUrls.push(presignedUrl);
    }

    console.log(`[${activityId}] ‚úÖ Chunk ${chunkIndex} completed:`);
    console.log(`  - Pages: ${startPage}-${endPage}`);
    console.log(`  - Processing Time: ${result.processingTimeMs}ms`);
    console.log(`  - Generated ${imagePresignedUrls.length} presigned URLs`);

    return {
      imagePresignedUrls,
      pageCount: result.totalPages,
      processingTimeMs: result.processingTimeMs,
      bucket
    };

  } catch (error) {
    console.error(`[${activityId}] ‚ùå PDF page range conversion failed:`, error);
    throw new Error(`PDF page range conversion failed: ${error instanceof Error ? error.message : 'Unknown error'}`);
  }
}


